#!/usr/bin/env python3
"""
í…ìŠ¤íŠ¸ ì²­í‚¹ ì›ë¦¬ë¥¼ ì‹œê°í™”í•˜ëŠ” ì˜ˆì œ
"""

def visualize_chunking_process():
    """í…ìŠ¤íŠ¸ ì²­í‚¹ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹œê°í™”"""
    
    print("ğŸ¯ í…ìŠ¤íŠ¸ ì²­í‚¹ ê³¼ì • ì‹œê°í™”")
    print("=" * 60)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸
    original_text = """SJH-Style FloorPlan Generation

[Number and Type of Rooms]
The floorplan have 1 living room, 1 master room, 1 kitchen, 1 bathroom

[Connection Between Rooms]
living room #1 and master room #1 are connected.
living room #1 and kitchen #1 are connected.
living room #1 and bathroom #1 are connected.

[Positional Relationship Between Rooms]
master room #1 is left-below living room #1.
kitchen #1 is above living room #1.
bathroom #1 is left-of living room #1."""
    
    print("ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸:")
    print(f"ê¸¸ì´: {len(original_text.split())} ë‹¨ì–´")
    print(original_text)
    print()
    
    # ì²­í‚¹ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
    chunks = [
        "[Number and Type of Rooms] The floorplan have 1 living room, 1 master room, 1 kitchen, 1 bathroom",
        "[Connection Between Rooms] living room #1 and master room #1 are connected. living room #1 and kitchen #1 are connected.",
        "[Connection Between Rooms] living room #1 and bathroom #1 are connected.",
        "[Positional Relationship Between Rooms] master room #1 is left-below living room #1. kitchen #1 is above living room #1.",
        "[Positional Relationship Between Rooms] bathroom #1 is left-of living room #1."
    ]
    
    print("ğŸ§© ì²­í‚¹ ê²°ê³¼:")
    for i, chunk in enumerate(chunks, 1):
        print(f"ì²­í¬ {i}: {chunk}")
        print(f"    â””â”€ ê¸¸ì´: {len(chunk.split())} ë‹¨ì–´")
    print()
    
    return chunks

def simulate_embedding_process(chunks):
    """ì„ë² ë”© ê³¼ì • ì‹œë®¬ë ˆì´ì…˜"""
    print("ğŸ”¢ ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜:")
    print("=" * 60)
    
    import numpy as np
    
    # ê° ì²­í¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í–ˆë‹¤ê³  ê°€ì • (ì‹¤ì œë¡œëŠ” 768ì°¨ì›)
    # ì—¬ê¸°ì„œëŠ” ì‹œê°í™”ë¥¼ ìœ„í•´ 3ì°¨ì›ìœ¼ë¡œ ë‹¨ìˆœí™”
    embeddings = []
    
    for i, chunk in enumerate(chunks, 1):
        # ì˜ë¯¸ë¥¼ ë°˜ì˜í•œ ê°€ìƒì˜ ì„ë² ë”© ë²¡í„°
        if "Number and Type" in chunk:
            # ë°© ì •ë³´ - êµ¬ì¡°ì  ì •ë³´ ê°•í•¨
            embedding = np.array([0.8, 0.2, 0.1])
        elif "Connection Between" in chunk:
            # ì—°ê²° ì •ë³´ - ê´€ê³„ ì •ë³´ ê°•í•¨
            embedding = np.array([0.3, 0.9, 0.2])
        elif "Positional Relationship" in chunk:
            # ìœ„ì¹˜ ì •ë³´ - ê³µê°„ ì •ë³´ ê°•í•¨
            embedding = np.array([0.2, 0.3, 0.8])
        else:
            embedding = np.array([0.5, 0.5, 0.5])
        
        embeddings.append(embedding)
        print(f"ì²­í¬ {i} â†’ ë²¡í„°: [{embedding[0]:.1f}, {embedding[1]:.1f}, {embedding[2]:.1f}]")
        print(f"    â””â”€ íŠ¹ì„±: {'êµ¬ì¡°' if embedding[0] > 0.5 else 'ê´€ê³„' if embedding[1] > 0.5 else 'ê³µê°„'}")
    
    print()
    return np.array(embeddings)

def simple_attention_explanation():
    """ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…"""
    print("ğŸ§  ì–´í…ì…˜ì´ë€? (ì‰¬ìš´ ì˜ˆì‹œ)")
    print("=" * 60)
    
    print("ğŸ“š ìƒí™©: ì—¬ëŸ¬ ì±…ì—ì„œ ì •ë³´ë¥¼ ì¢…í•©í•´ì•¼ í•¨")
    print()
    print("ì±… 1: 'ë°©ì˜ ì¢…ë¥˜ì™€ ê°œìˆ˜' (ë§¤ìš° ì¤‘ìš”)")
    print("ì±… 2: 'ë°© ê°„ ì—°ê²°ê´€ê³„' (ì¤‘ìš”)")  
    print("ì±… 3: 'ë°©ì˜ ìœ„ì¹˜ê´€ê³„' (ì¤‘ìš”)")
    print("ì±… 4: 'ê¸°íƒ€ ì •ë³´' (ëœ ì¤‘ìš”)")
    print()
    
    print("ğŸ¤” ì§ˆë¬¸: 'ì´ í‰ë©´ë„ì˜ í•µì‹¬ íŠ¹ì§•ì€?'")
    print()
    print("ğŸ¯ ì–´í…ì…˜ì˜ ì—­í• :")
    print("1. ê° ì±…ì˜ ì¤‘ìš”ë„ë¥¼ ìë™ìœ¼ë¡œ íŒë‹¨")
    print("2. ì¤‘ìš”í•œ ì±…ì— ë” ë§ì€ 'ì£¼ì˜'(attention)ë¥¼ ê¸°ìš¸ì„")
    print("3. ê°€ì¤‘í‰ê· ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±")
    print()
    
    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‹œë®¬ë ˆì´ì…˜
    attention_weights = [0.4, 0.3, 0.25, 0.05]  # í•©ê³„ = 1.0
    books = ["ë°© ì •ë³´", "ì—°ê²°ê´€ê³„", "ìœ„ì¹˜ê´€ê³„", "ê¸°íƒ€"]
    
    print("ğŸ“Š ì–´í…ì…˜ ê°€ì¤‘ì¹˜:")
    for book, weight in zip(books, attention_weights):
        bar = "â–ˆ" * int(weight * 20)  # ì‹œê°í™”
        print(f"{book:8s}: {weight:4.1%} {bar}")
    print()

def detailed_attention_mechanism():
    """ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ìƒì„¸í•œ ì‘ë™ ì›ë¦¬"""
    print("ğŸ”¬ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸ ì›ë¦¬")
    print("=" * 60)
    
    import numpy as np
    
    # ê°€ìƒì˜ ì²­í¬ ì„ë² ë”© (3ê°œ ì²­í¬, ê°ê° 4ì°¨ì›)
    embeddings = np.array([
        [0.8, 0.2, 0.1, 0.3],  # ì²­í¬ 1: ë°© ì •ë³´
        [0.3, 0.9, 0.2, 0.4],  # ì²­í¬ 2: ì—°ê²°ê´€ê³„
        [0.2, 0.3, 0.8, 0.5],  # ì²­í¬ 3: ìœ„ì¹˜ê´€ê³„
    ])
    
    print("ğŸ“Š ì…ë ¥ ì²­í¬ ì„ë² ë”©:")
    print("ì²­í¬ 1 (ë°© ì •ë³´):", embeddings[0])
    print("ì²­í¬ 2 (ì—°ê²°ê´€ê³„):", embeddings[1]) 
    print("ì²­í¬ 3 (ìœ„ì¹˜ê´€ê³„):", embeddings[2])
    print()
    
    print("ğŸ” 1ë‹¨ê³„: Query ìƒì„± (ì§ˆë¬¸ ë§Œë“¤ê¸°)")
    # ê° ì²­í¬ì˜ í‰ê· ì„ Queryë¡œ ì‚¬ìš©
    query = embeddings.mean(axis=1, keepdims=True)  # ê° ì²­í¬ë³„ í‰ê· 
    print("ê° ì²­í¬ì˜ ì§ˆë¬¸(Query):")
    for i, q in enumerate(query):
        print(f"ì²­í¬ {i+1} Query: {q[0]}")
    print()
    
    print("ğŸ” 2ë‹¨ê³„: Key ìƒì„± (ë‹µë³€ ì¤€ë¹„)")
    # ê° ì²­í¬ì˜ í‰ê· ì„ Keyë¡œ ì‚¬ìš©
    key = embeddings.mean(axis=1)  # ê° ì²­í¬ì˜ íŠ¹ì„±
    print("ê° ì²­í¬ì˜ íŠ¹ì„±(Key):")
    for i, k in enumerate(key):
        print(f"ì²­í¬ {i+1} Key: {k:.3f}")
    print()
    
    print("ğŸ” 3ë‹¨ê³„: ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°")
    print("(ê° Queryì™€ ëª¨ë“  Key ê°„ì˜ ìœ ì‚¬ë„)")
    
    # ì–´í…ì…˜ ìŠ¤ì½”ì–´ = Query @ Key^T
    attention_scores = np.dot(query.squeeze(), key.T)
    print("ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬:")
    print(attention_scores)
    print()
    
    print("ğŸ” 4ë‹¨ê³„: Softmaxë¡œ í™•ë¥  ë³€í™˜")
    
    # ìˆ˜ë™ìœ¼ë¡œ softmax êµ¬í˜„ (ë” ì•ˆì „í•¨)
    def manual_softmax(x):
        exp_x = np.exp(x - np.max(x))  # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ê°’ ë¹¼ê¸°
        return exp_x / np.sum(exp_x)
    
    # ê° í–‰ì— ëŒ€í•´ softmax ì ìš©
    attention_weights = np.zeros_like(attention_scores)
    for i in range(attention_scores.shape[0]):
        attention_weights[i] = manual_softmax(attention_scores[i])
    
    print("ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ê° í–‰ì˜ í•© = 1.0):")
    for i, weights in enumerate(attention_weights):
        print(f"ì²­í¬ {i+1}ì´ ë‹¤ë¥¸ ì²­í¬ë“¤ì— ì£¼ëŠ” ê´€ì‹¬:")
        for j, weight in enumerate(weights):
            print(f"  â†’ ì²­í¬ {j+1}: {weight:.3f}")
    print()
    
    print("ğŸ” 5ë‹¨ê³„: ê°€ì¤‘í‰ê· ìœ¼ë¡œ ìµœì¢… ê²°ê³¼")
    # ê° ì²­í¬ì— ëŒ€í•´ ê°€ì¤‘í‰ê·  ê³„ì‚°
    final_embeddings = np.zeros_like(embeddings)
    
    for i in range(len(embeddings)):
        weighted_sum = np.zeros(embeddings.shape[1])
        for j in range(len(embeddings)):
            weighted_sum += attention_weights[i, j] * embeddings[j]
        final_embeddings[i] = weighted_sum
        
        print(f"ì²­í¬ {i+1} ìµœì¢… ì„ë² ë”©: {final_embeddings[i]}")
    
    return final_embeddings

def practical_example():
    """ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ"""
    print("\nğŸ  ì‹¤ì œ í‰ë©´ë„ ìƒì„± ì˜ˆì‹œ")
    print("=" * 60)
    
    print("ğŸ“ ì…ë ¥: 'ê±°ì‹¤, ì¹¨ì‹¤, ì£¼ë°©ì´ ìˆê³  ê±°ì‹¤ê³¼ ì¹¨ì‹¤ì´ ì—°ê²°ëœ í‰ë©´ë„'")
    print()
    
    print("ğŸ§© ì²­í‚¹ ê³¼ì •:")
    print("ì²­í¬ 1: 'ê±°ì‹¤, ì¹¨ì‹¤, ì£¼ë°©ì´ ìˆê³ '")
    print("ì²­í¬ 2: 'ê±°ì‹¤ê³¼ ì¹¨ì‹¤ì´ ì—°ê²°ëœ'")
    print("ì²­í¬ 3: 'í‰ë©´ë„'")
    print()
    
    print("ğŸ§  ì–´í…ì…˜ ì²˜ë¦¬:")
    print("1. ì²­í¬ 1 (ë°© ì •ë³´): ê°€ì¤‘ì¹˜ 0.5 (ë§¤ìš° ì¤‘ìš”)")
    print("2. ì²­í¬ 2 (ì—°ê²° ì •ë³´): ê°€ì¤‘ì¹˜ 0.4 (ì¤‘ìš”)")  
    print("3. ì²­í¬ 3 (ì¼ë°˜ ì •ë³´): ê°€ì¤‘ì¹˜ 0.1 (ëœ ì¤‘ìš”)")
    print()
    
    print("ğŸ¨ ê²°ê³¼:")
    print("â†’ ê±°ì‹¤, ì¹¨ì‹¤, ì£¼ë°©ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ í‰ë©´ë„")
    print("â†’ ê±°ì‹¤-ì¹¨ì‹¤ ì—°ê²°ì„ ê°•ì¡°")
    print("â†’ ì „ì²´ì ì¸ í‰ë©´ë„ í˜•íƒœ ìœ ì§€")

if __name__ == "__main__":
    chunks = visualize_chunking_process()
    embeddings = simulate_embedding_process(chunks)
    simple_attention_explanation()
    detailed_attention_mechanism()
    practical_example()
