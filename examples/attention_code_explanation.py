#!/usr/bin/env python3
"""
ì‹¤ì œ _attention_aggregation() ì½”ë“œë¥¼ í•œ ì¤„ì”© ë¶„ì„
"""

import torch
import torch.nn.functional as F
import numpy as np

def explain_attention_aggregation():
    """ì‹¤ì œ ì½”ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…"""
    print("ğŸ”¬ ì‹¤ì œ _attention_aggregation() ì½”ë“œ ë¶„ì„")
    print("=" * 60)
    print()
    
    # ê°€ìƒì˜ ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    batch_size, num_chunks, seq_len, hidden_dim = 1, 3, 77, 768
    embeddings = torch.randn(batch_size, num_chunks, seq_len, hidden_dim)
    
    print("ğŸ“Š ì…ë ¥ ë°ì´í„°:")
    print(f"embeddings í˜•íƒœ: {embeddings.shape}")
    print("â†’ (ë°°ì¹˜í¬ê¸°, ì²­í¬ìˆ˜, ì‹œí€€ìŠ¤ê¸¸ì´, íˆë“ ì°¨ì›)")
    print(f"â†’ ({batch_size}, {num_chunks}, {seq_len}, {hidden_dim})")
    print()
    
    print("=" * 60)
    print("ğŸ§  ì½”ë“œ í•œ ì¤„ì”© ë¶„ì„")
    print("=" * 60)
    print()
    
    # ì‹¤ì œ ì½”ë“œ ë¼ì¸ë³„ ë¶„ì„
    print("1ï¸âƒ£ ì…ë ¥ í˜•íƒœ í™•ì¸")
    print("```python")
    print("batch_size, seq_len, hidden_dim = embeddings.shape")
    print("```")
    batch_size, seq_len, hidden_dim = embeddings.shape[0], embeddings.shape[2], embeddings.shape[3]
    print(f"ê²°ê³¼: batch_size={batch_size}, seq_len={seq_len}, hidden_dim={hidden_dim}")
    print("ğŸ’¡ 3ì°¨ì›ì—ì„œ ê° ì°¨ì›ì˜ ì˜ë¯¸ë¥¼ ì¶”ì¶œ")
    print()
    
    print("2ï¸âƒ£ Query ìƒì„± (ê°„ë‹¨í•œ self-attention)")
    print("```python")
    print("q = embeddings.mean(dim=1, keepdim=True)")
    print("```")
    q = embeddings.mean(dim=1, keepdim=True)  # (batch_size, 1, seq_len, hidden_dim)
    print(f"ê²°ê³¼ í˜•íƒœ: {q.shape}")
    print("ğŸ’¡ ëª¨ë“  ì²­í¬ì˜ í‰ê· ì„ Queryë¡œ ì‚¬ìš©")
    print("   â†’ 'ì „ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ê°€ ì¤‘ìš”í•œê°€?'ë¼ëŠ” ì§ˆë¬¸")
    print()
    
    print("3ï¸âƒ£ Key ìƒì„±")
    print("```python")
    print("k = embeddings.mean(dim=1)")
    print("```")
    k = embeddings.mean(dim=1)  # (batch_size, seq_len, hidden_dim)
    print(f"ê²°ê³¼ í˜•íƒœ: {k.shape}")
    print("ğŸ’¡ ê° ì²­í¬ì˜ íŠ¹ì„±ì„ Keyë¡œ ì‚¬ìš©")
    print("   â†’ 'ê° ì²­í¬ëŠ” ì–´ë–¤ íŠ¹ì„±ì„ ê°€ì§€ëŠ”ê°€?'")
    print()
    
    print("4ï¸âƒ£ ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°")
    print("```python")
    print("attention_scores = torch.matmul(q.squeeze(1), k.transpose(-2, -1))")
    print("```")
    attention_scores = torch.matmul(q.squeeze(1), k.transpose(-2, -1))
    print(f"ê²°ê³¼ í˜•íƒœ: {attention_scores.shape}")
    print("ğŸ’¡ Queryì™€ Keyì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°")
    print("   â†’ ë†’ì€ ê°’ = ë” ê´€ë ¨ì„± ìˆìŒ")
    print()
    
    print("5ï¸âƒ£ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°")
    print("```python")
    print("attention_weights = F.softmax(attention_scores.mean(dim=1), dim=0)")
    print("```")
    attention_weights = F.softmax(attention_scores.mean(dim=1), dim=0)
    print(f"ê²°ê³¼ í˜•íƒœ: {attention_weights.shape}")
    print("ğŸ’¡ Softmaxë¡œ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜")
    print(f"   â†’ ê°€ì¤‘ì¹˜ í•©ê³„: {attention_weights.sum():.3f} (í•­ìƒ 1.0)")
    print()
    
    print("6ï¸âƒ£ ê°€ì¤‘ì¹˜ ì°¨ì› ì¡°ì •")
    print("```python")
    print("attention_weights = attention_weights.view(-1, 1, 1)")
    print("```")
    attention_weights_reshaped = attention_weights.view(-1, 1, 1)
    print(f"ê²°ê³¼ í˜•íƒœ: {attention_weights_reshaped.shape}")
    print("ğŸ’¡ ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•´ ì°¨ì› í™•ì¥")
    print("   â†’ ê° ì²­í¬ë³„ë¡œ ì „ì²´ ì„ë² ë”©ì— ê°€ì¤‘ì¹˜ ì ìš© ì¤€ë¹„")
    print()
    
    print("7ï¸âƒ£ ê°€ì¤‘ í‰ê·  ê³„ì‚°")
    print("```python")
    print("weighted_embeddings = embeddings * attention_weights")
    print("```")
    weighted_embeddings = embeddings * attention_weights_reshaped
    print(f"ê²°ê³¼ í˜•íƒœ: {weighted_embeddings.shape}")
    print("ğŸ’¡ ê° ì²­í¬ì— í•´ë‹¹ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•¨")
    print("   â†’ ì¤‘ìš”í•œ ì²­í¬ëŠ” ê°’ì´ ì»¤ì§€ê³ , ëœ ì¤‘ìš”í•œ ì²­í¬ëŠ” ê°’ì´ ì‘ì•„ì§")
    print()
    
    print("8ï¸âƒ£ ìµœì¢… ê²°í•©")
    print("```python")
    print("return weighted_embeddings.sum(dim=0)")
    print("```")
    final_result = weighted_embeddings.sum(dim=0)
    print(f"ê²°ê³¼ í˜•íƒœ: {final_result.shape}")
    print("ğŸ’¡ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ëª¨ë“  ì²­í¬ë¥¼ í•©ì‚°")
    print("   â†’ ìµœì¢… í†µí•© ì„ë² ë”© ì™„ì„±!")
    print()
    
    return final_result

def visual_example():
    """ì‹œê°ì  ì˜ˆì œë¡œ ì„¤ëª…"""
    print("ğŸ¨ ì‹œê°ì  ì˜ˆì œ")
    print("=" * 60)
    print()
    
    print("ìƒí™©: 3ê°œ ì²­í¬ê°€ ìˆë‹¤ê³  ê°€ì •")
    print()
    
    # ê°„ë‹¨í•œ ì˜ˆì œ ë°ì´í„°
    embeddings = torch.tensor([
        [[0.8, 0.1, 0.1]],  # ì²­í¬ 1: êµ¬ì¡° ì •ë³´ ê°•í•¨
        [[0.1, 0.8, 0.1]],  # ì²­í¬ 2: ê´€ê³„ ì •ë³´ ê°•í•¨  
        [[0.1, 0.1, 0.8]]   # ì²­í¬ 3: ê³µê°„ ì •ë³´ ê°•í•¨
    ]).unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    
    print("ğŸ“Š ì…ë ¥ ì²­í¬ë“¤:")
    for i in range(3):
        chunk = embeddings[0, i, 0]
        print(f"ì²­í¬ {i+1}: [{chunk[0]:.1f}, {chunk[1]:.1f}, {chunk[2]:.1f}] - ", end="")
        if chunk[0] > 0.5:
            print("êµ¬ì¡° ì •ë³´ ì¤‘ì‹¬")
        elif chunk[1] > 0.5:
            print("ê´€ê³„ ì •ë³´ ì¤‘ì‹¬")
        else:
            print("ê³µê°„ ì •ë³´ ì¤‘ì‹¬")
    print()
    
    # ì–´í…ì…˜ ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜
    q = embeddings.mean(dim=1, keepdim=True)  # ì „ì²´ í‰ê· ì„ Queryë¡œ
    k = embeddings.mean(dim=2)  # ê° ì²­í¬ì˜ íŠ¹ì„±ì„ Keyë¡œ
    
    print("ğŸ” ì–´í…ì…˜ ê³„ì‚°:")
    print(f"Query (ì „ì²´ í‰ê· ): [{q[0,0,0,0]:.2f}, {q[0,0,0,1]:.2f}, {q[0,0,0,2]:.2f}]")
    print()
    
    # ê° ì²­í¬ì™€ Queryì˜ ìœ ì‚¬ë„
    similarities = []
    for i in range(3):
        key = k[0, i]
        similarity = torch.dot(q[0,0,0], key).item()
        similarities.append(similarity)
        print(f"ì²­í¬ {i+1}ê³¼ Query ìœ ì‚¬ë„: {similarity:.3f}")
    
    # Softmaxë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    similarities_tensor = torch.tensor(similarities)
    weights = F.softmax(similarities_tensor, dim=0)
    
    print()
    print("ğŸ“Š ì–´í…ì…˜ ê°€ì¤‘ì¹˜:")
    for i, weight in enumerate(weights):
        bar = "â–ˆ" * int(weight * 20)
        print(f"ì²­í¬ {i+1}: {weight:.3f} ({weight*100:.1f}%) {bar}")
    
    # ìµœì¢… ê²°ê³¼ ê³„ì‚°
    weighted_sum = torch.zeros(3)
    for i in range(3):
        weighted_chunk = embeddings[0, i, 0] * weights[i]
        weighted_sum += weighted_chunk
        print(f"ì²­í¬ {i+1} ê¸°ì—¬ë¶„: [{weighted_chunk[0]:.3f}, {weighted_chunk[1]:.3f}, {weighted_chunk[2]:.3f}]")
    
    print()
    print(f"ğŸ¯ ìµœì¢… ê²°ê³¼: [{weighted_sum[0]:.3f}, {weighted_sum[1]:.3f}, {weighted_sum[2]:.3f}]")
    print("ğŸ’¡ ëª¨ë“  ì •ë³´ê°€ ì ì ˆíˆ í˜¼í•©ëœ í†µí•© ì„ë² ë”©!")

def why_attention_works():
    """ì™œ ì–´í…ì…˜ì´ íš¨ê³¼ì ì¸ì§€ ì„¤ëª…"""
    print()
    print("ğŸ¤” ì™œ ì–´í…ì…˜ì´ íš¨ê³¼ì ì¸ê°€?")
    print("=" * 60)
    print()
    
    print("1ï¸âƒ£ ë™ì  ê°€ì¤‘ì¹˜")
    print("   â€¢ ê³ ì •ëœ í‰ê· ì´ ì•„ë‹Œ, ë‚´ìš©ì— ë”°ë¼ ê°€ì¤‘ì¹˜ê°€ ë‹¬ë¼ì§")
    print("   â€¢ ì¤‘ìš”í•œ ì •ë³´ëŠ” ìë™ìœ¼ë¡œ ë” í° ì˜í–¥ë ¥ì„ ê°€ì§")
    print()
    
    print("2ï¸âƒ£ ë§¥ë½ ì¸ì‹")
    print("   â€¢ ê° ì²­í¬ê°€ ë‹¤ë¥¸ ì²­í¬ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ê³ ë ¤")
    print("   â€¢ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë“¤ë¼ë¦¬ ì„œë¡œ ê°•í™”")
    print()
    
    print("3ï¸âƒ£ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”")
    print("   â€¢ ëª¨ë“  ì²­í¬ê°€ ìµœì¢… ê²°ê³¼ì— ê¸°ì—¬")
    print("   â€¢ ë‹¨ìˆœ ìë¥´ê¸° ëŒ€ë¹„ ì •ë³´ ë³´ì¡´ìœ¨ 100%")
    print()
    
    print("4ï¸âƒ£ í™•ì¥ì„±")
    print("   â€¢ ì²­í¬ ê°œìˆ˜ì— ê´€ê³„ì—†ì´ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬")
    print("   â€¢ ìƒˆë¡œìš´ ì„¹ì…˜ ì¶”ê°€ì—ë„ ìë™ ì ì‘")
    print()
    
    print("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:")
    methods = [
        ("ê¸°ë³¸ ìë¥´ê¸°", 0.3, "ğŸ”´"),
        ("ìŠ¤ë§ˆíŠ¸ ìë¥´ê¸°", 0.6, "ğŸŸ¡"), 
        ("ë‹¨ìˆœ í‰ê· ", 0.7, "ğŸŸ¢"),
        ("ì–´í…ì…˜ ì§‘ê³„", 0.9, "ğŸŸ¢")
    ]
    
    for method, score, color in methods:
        bar = "â–ˆ" * int(score * 10)
        print(f"{method:12s}: {score:.1f} {color} {bar}")

def practical_tips():
    """ì‹¤ì œ ì‚¬ìš© íŒ"""
    print()
    print("ğŸ’¡ ì‹¤ì œ ì‚¬ìš© íŒ")
    print("=" * 60)
    print()
    
    print("ğŸ”§ íŒŒë¼ë¯¸í„° ì¡°ì •:")
    print("â€¢ aggregation_method='attention': ê°€ì¥ ì§€ëŠ¥ì ")
    print("â€¢ aggregation_method='weighted': ì¤‘ê°„ ë³µì¡ë„")
    print("â€¢ aggregation_method='mean': ê°€ì¥ ê°„ë‹¨")
    print()
    
    print("âš¡ ì„±ëŠ¥ ìµœì í™”:")
    print("â€¢ ì²­í¬ ìˆ˜ê°€ ë§ì„ ë•ŒëŠ” 'mean' ì‚¬ìš© ê³ ë ¤")
    print("â€¢ í’ˆì§ˆì´ ì¤‘ìš”í•  ë•ŒëŠ” 'attention' ì‚¬ìš©")
    print("â€¢ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œ 'weighted' ì‚¬ìš©")
    print()
    
    print("ğŸ¯ ì ìš© ì‹œì :")
    print("â€¢ í† í° ê¸¸ì´ > 100: attention ê¶Œì¥")
    print("â€¢ í† í° ê¸¸ì´ 77~100: weighted ì‚¬ìš©")
    print("â€¢ í† í° ê¸¸ì´ < 77: ê¸°ë³¸ ë°©ë²• ì‚¬ìš©")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“ _attention_aggregation() ì™„ì „ ë¶„ì„")
    print("=" * 60)
    print()
    
    final_result = explain_attention_aggregation()
    visual_example()
    why_attention_works()
    practical_tips()
    
    print()
    print("ğŸ‰ í•µì‹¬ ìš”ì•½:")
    print("_attention_aggregation()ì€ ì—¬ëŸ¬ ì²­í¬ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬")
    print("ê¸´ í…ìŠ¤íŠ¸ì˜ ëª¨ë“  ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë³´ì¡´í•˜ëŠ” í˜ì‹ ì ì¸ ë°©ë²•!")

if __name__ == "__main__":
    main()
