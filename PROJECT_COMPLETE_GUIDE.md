# í…ìŠ¤íŠ¸ ì¡°ê±´ í‰ë©´ë„ ìƒì„± ëª¨ë¸ - ì™„ì „ ê°€ì´ë“œ

> **ìƒˆë¡œìš´ ì„¸ì…˜ì„ ìœ„í•œ ì¢…í•© ì„¤ëª…ì„œ**  
> ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­, êµ¬í˜„ ë‚´ìš©, ë°œìƒí–ˆë˜ ë¬¸ì œë“¤ê³¼ í•´ê²°ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ](#2-í•µì‹¬-ê¸°ìˆ -ìŠ¤íƒ)
3. [í”„ë¡œì íŠ¸ ì•„í‚¤í…ì²˜](#3-í”„ë¡œì íŠ¸-ì•„í‚¤í…ì²˜)
4. [ì£¼ìš” êµ¬í˜„ ì‚¬í•­](#4-ì£¼ìš”-êµ¬í˜„-ì‚¬í•­)
5. [ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#5-ë°ì´í„°-ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)
6. [ëª¨ë¸ êµ¬ì¡°](#6-ëª¨ë¸-êµ¬ì¡°)
7. [í›ˆë ¨ ì‹œìŠ¤í…œ](#7-í›ˆë ¨-ì‹œìŠ¤í…œ)
8. [ì¶”ë¡  ì‹œìŠ¤í…œ](#8-ì¶”ë¡ -ì‹œìŠ¤í…œ)
9. [ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ](#9-ë¶„ì‚°-í•™ìŠµ-ì‹œìŠ¤í…œ)
10. [í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê³ ê¸‰ ê¸°ëŠ¥](#10-í…ìŠ¤íŠ¸-ì²˜ë¦¬-ê³ ê¸‰-ê¸°ëŠ¥)
11. [ë°œìƒí•œ ë¬¸ì œë“¤ê³¼ í•´ê²°ë°©ë²•](#11-ë°œìƒí•œ-ë¬¸ì œë“¤ê³¼-í•´ê²°ë°©ë²•)
12. [íŒŒì¼ë³„ ìƒì„¸ ì„¤ëª…](#12-íŒŒì¼ë³„-ìƒì„¸-ì„¤ëª…)
13. [ì„¤ì • ë° ì‹¤í–‰ ë°©ë²•](#13-ì„¤ì •-ë°-ì‹¤í–‰-ë°©ë²•)
14. [ê°œë°œ íˆìŠ¤í† ë¦¬](#14-ê°œë°œ-íˆìŠ¤í† ë¦¬)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ
- **ì…ë ¥**: êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ (ë°©ì˜ ê°œìˆ˜/ì¢…ë¥˜, ì—°ê²°ê´€ê³„, ìœ„ì¹˜ê´€ê³„)
- **ì¶œë ¥**: 256x256 RGB í‰ë©´ë„ ì´ë¯¸ì§€
- **íŠ¹ì§•**: LoRA íŒŒì¸íŠœë‹ì„ í†µí•œ Stable Diffusion ê¸°ë°˜ ìƒì„± ëª¨ë¸

### 1.2 í•µì‹¬ í˜ì‹ ì‚¬í•­
1. **Attention ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹**: CLIP 77í† í° ì œí•œ ê·¹ë³µ
2. **2ë‹¨ê³„ ë¶„ì‚°í•™ìŠµ**: Windows í™˜ê²½ ìµœì í™” ìë™í™” ì‹œìŠ¤í…œ
3. **ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì¦ê°•**: ë¶€ë¶„ ì¡°ê±´ ì…ë ¥ ì§€ì›

### 1.3 ì…ë ¥ í…ìŠ¤íŠ¸ í˜•ì‹
```
SJH-Style FloorPlan Generation

[Number and Type of Rooms]
The floorplan have 1 living room, 1 master room, 1 kitchen, 1 bathroom

[Connection Between Rooms]
living room #1 and master room #1 are connected.
living room #1 and kitchen #1 are connected.

[Positional Relationship Between Rooms]
master room #1 is left-below living room #1.
kitchen #1 is above living room #1.
```

### 1.4 ì¶œë ¥ ì´ë¯¸ì§€ í˜•ì‹
- **í•´ìƒë„**: 256x256x3 (RGB)
- **ìƒ‰ìƒ ë§¤í•‘**: ê° ë°© ìœ í˜•ë³„ ê³ ìœ  RGB ê°’
```python
room_colors = {
    'living_room': [255, 255, 220],    # ì—°í•œ ë…¸ë€ìƒ‰
    'master_room': [0, 255, 0],        # ì´ˆë¡ìƒ‰
    'bedroom': [30, 140, 50],          # ì–´ë‘ìš´ ì´ˆë¡ìƒ‰
    'kitchen': [190, 90, 90],          # ê°ˆìƒ‰
    'bathroom': [66, 78, 255],         # íŒŒë€ìƒ‰
    'balcony': [50, 180, 255],         # í•˜ëŠ˜ìƒ‰
    'wall': [95, 95, 95],              # íšŒìƒ‰
    'door': [255, 255, 0],             # ë…¸ë€ìƒ‰
    'window': [125, 190, 190]          # ì²­ë¡ìƒ‰
}
```

---

## 2. í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

### 2.1 ê¸°ë°˜ ê¸°ìˆ 
- **Python**: 3.9+
- **PyTorch**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **Stable Diffusion v1.5**: ê¸°ë°˜ ìƒì„± ëª¨ë¸
- **LoRA (Low-Rank Adaptation)**: íš¨ìœ¨ì  íŒŒì¸íŠœë‹
- **CLIP**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì¸ì½”ë”©

### 2.2 ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Hugging Face Accelerate**: ë¶„ì‚° í•™ìŠµ ë° í˜¼í•© ì •ë°€ë„
- **Diffusers**: Stable Diffusion íŒŒì´í”„ë¼ì¸
- **PEFT**: LoRA êµ¬í˜„
- **Weights & Biases**: ì‹¤í—˜ ì¶”ì 
- **OpenCV**: ì´ë¯¸ì§€ í›„ì²˜ë¦¬
- **uv**: Python íŒ¨í‚¤ì§€ ê´€ë¦¬

### 2.3 ê°œë°œ ë„êµ¬
- **PowerShell**: Windows ë¶„ì‚°í•™ìŠµ ìë™í™”
- **YAML**: ì„¤ì • íŒŒì¼ í˜•ì‹
- **Git**: ë²„ì „ ê´€ë¦¬

---

## 3. í”„ë¡œì íŠ¸ ì•„í‚¤í…ì²˜

### 3.1 ì „ì²´ íŒŒì´í”„ë¼ì¸
```
í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ì „ì²˜ë¦¬/ì²­í‚¹ â†’ CLIP ì¸ì½”ë”© â†’ U-Net ë””í“¨ì „ â†’ VAE ë””ì½”ë”© â†’ í›„ì²˜ë¦¬ â†’ ìµœì¢… ì´ë¯¸ì§€
```

### 3.2 ëª¨ë“ˆ êµ¬ì¡°
```
src/
â”œâ”€â”€ data/           # ë°ì´í„° ì²˜ë¦¬
â”œâ”€â”€ models/         # ëª¨ë¸ ì •ì˜
â”œâ”€â”€ training/       # í›ˆë ¨ ì‹œìŠ¤í…œ
â”œâ”€â”€ inference/      # ì¶”ë¡  ì‹œìŠ¤í…œ
â””â”€â”€ utils/          # ìœ í‹¸ë¦¬í‹°
```

### 3.3 ì£¼ìš” ì»´í¬ë„ŒíŠ¸
1. **FloorPlanDataset**: ë°ì´í„° ë¡œë”© ë° ì¦ê°•
2. **FloorPlanDiffusionModel**: LoRA ì ìš©ëœ Stable Diffusion
3. **FloorPlanTrainer**: Accelerate ê¸°ë°˜ í†µí•© í›ˆë ¨ê¸°
4. **AccelerateFloorPlanGenerator**: ì¶”ë¡  ì—”ì§„
5. **FloorPlanPostProcessor**: ì´ë¯¸ì§€ í›„ì²˜ë¦¬

---

## 4. ì£¼ìš” êµ¬í˜„ ì‚¬í•­

### 4.1 LoRA íŒŒì¸íŠœë‹
```python
# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,                    # rank
    lora_alpha=32,          # alpha
    target_modules=["to_q", "to_k", "to_v", "to_out.0"],
    lora_dropout=0.1
)
```

### 4.2 VAE ì¸ì½”ë”©/ë””ì½”ë”©
- **ì¸ì½”ë”**: 256Ã—256Ã—3 â†’ 32Ã—32Ã—4 (16ë°° ì••ì¶•)
- **ë””ì½”ë”**: 32Ã—32Ã—4 â†’ 256Ã—256Ã—3
- **ëª©ì **: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë° í•™ìŠµ ì•ˆì •ì„±

### 4.3 í˜¼í•© ì •ë°€ë„ í›ˆë ¨
```yaml
mixed_precision: fp16  # ë©”ëª¨ë¦¬ ì ˆì•½
```

### 4.4 ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
```yaml
gradient_accumulation_steps: 4  # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ì¦ê°€
```

---

## 5. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 5.1 ë°ì´í„° êµ¬ì¡°
```
data/train/
â”œâ”€â”€ input_text/        # í…ìŠ¤íŠ¸ íŒŒì¼ (n.txt)
â””â”€â”€ label_floorplan/   # ì´ë¯¸ì§€ íŒŒì¼ (n.png)
```

### 5.2 ë°ì´í„° ì¦ê°• ì „ëµ

#### 5.2.1 ë¬´ì‘ìœ„ ë§ˆìŠ¤í‚¹
```python
# ê° ì„¹ì…˜ì—ì„œ 20-50% í•­ëª© ë¬´ì‘ìœ„ ì œê±°
room_mask_prob: 0.3
connection_mask_prob: 0.3
position_mask_prob: 0.3
```

#### 5.2.2 ìˆœì„œ ë³€ê²½
```python
# ì—°ê²°ê´€ê³„: "A and B connected" â†” "B and A connected"
# ìœ„ì¹˜ê´€ê³„: "A is left-below B" â†” "B is right-above A"
swap_prob: 0.5
```

#### 5.2.3 ìœ„ì¹˜ ê´€ê³„ ëŒ€ì¹­ì„±
```python
position_mappings = {
    "left": "right", "right": "left",
    "above": "below", "below": "above",
    "left-below": "right-above",
    "left-above": "right-below"
}
```

### 5.3 í† í°í™” ê³¼ì •
```python
# CLIP í† í°í™” (ê¸°ë³¸ 77í† í° ì œí•œ)
tokenizer(text, max_length=77, truncation=True, return_tensors="pt")
```

---

## 6. ëª¨ë¸ êµ¬ì¡°

### 6.1 Stable Diffusion v1.5 êµ¬ì„±
```
1. Text Encoder: CLIP ViT-L/14 (768dim ì„ë² ë”©)
2. U-Net: 860M parameters (ë…¸ì´ì¦ˆ ì˜ˆì¸¡)
3. VAE: 84M parameters (ì´ë¯¸ì§€ â†” ì ì¬ê³µê°„ ë³€í™˜)
```

### 6.2 LoRA ì ìš© ëŒ€ìƒ
```python
# U-Net
target_modules = ["to_q", "to_k", "to_v", "to_out.0"]

# Text Encoder  
target_modules = ["q_proj", "v_proj"]
```

### 6.3 ì†ì‹¤ í•¨ìˆ˜
```python
# MSE Loss (ë…¸ì´ì¦ˆ ì˜ˆì¸¡)
loss = F.mse_loss(noise_pred, noise)
```

---

## 7. í›ˆë ¨ ì‹œìŠ¤í…œ

### 7.1 FloorPlanTrainer í´ë˜ìŠ¤
- **ê¸°ë°˜**: Hugging Face Accelerate
- **ê¸°ëŠ¥**: ë¶„ì‚°í•™ìŠµ, í˜¼í•©ì •ë°€ë„, ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
- **ë¡œê¹…**: Weights & Biases ì—°ë™

### 7.2 í›ˆë ¨ ê³¼ì •
```python
1. VAE ì¸ì½”ë”©: ì´ë¯¸ì§€ â†’ ì ì¬ ê³µê°„
2. ë…¸ì´ì¦ˆ ì¶”ê°€: ìˆœë°©í–¥ ë””í“¨ì „
3. ë…¸ì´ì¦ˆ ì˜ˆì¸¡: U-Net ì¶”ë¡ 
4. ì†ì‹¤ ê³„ì‚°: MSE Loss
5. ì—­ì „íŒŒ: LoRA íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸
```

### 7.3 ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°
```yaml
learning_rate: 1e-4
batch_size: 2
num_epochs: 100
optimizer: AdamW
scheduler_type: cosine
warmup_steps: 1000
```

---

## 8. ì¶”ë¡  ì‹œìŠ¤í…œ

### 8.1 AccelerateFloorPlanGenerator
- **ì…ë ¥**: í…ìŠ¤íŠ¸ ì¡°ê±´, ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
- **ì¶œë ¥**: ìƒì„±ëœ í‰ë©´ë„ ì´ë¯¸ì§€
- **ê¸°ëŠ¥**: ë‹¨ì¼/ë°°ì¹˜/ëŒ€í™”í˜• ìƒì„±

### 8.2 ìƒì„± ê³¼ì •
```python
1. í…ìŠ¤íŠ¸ ì¸ì½”ë”©: CLIP â†’ ì„ë² ë”©
2. ì ì¬ ë…¸ì´ì¦ˆ: ëœë¤ ì´ˆê¸°í™”
3. ë””ë…¸ì´ì§•: U-Net ë°˜ë³µ ì‹¤í–‰
4. VAE ë””ì½”ë”©: ì ì¬ â†’ ì´ë¯¸ì§€
5. í›„ì²˜ë¦¬: ìƒ‰ìƒ ì •ê·œí™”
```

### 8.3 ìƒì„± íŒŒë¼ë¯¸í„°
```python
num_inference_steps: 50      # ì¶”ë¡  ìŠ¤í… ìˆ˜
guidance_scale: 7.5          # í…ìŠ¤íŠ¸ ê°€ì´ë˜ìŠ¤ ê°•ë„
seed: Optional[int]          # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
```

---

## 9. ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ

### 9.1 2ë‹¨ê³„ ë°©ì‹ (Windows ìµœì í™”)

#### 9.1.1 1ë‹¨ê³„: ì›Œì»¤ ì„¤ì • (scripts/worker_setup.ps1)
```powershell
# ê° ì›Œì»¤ ë…¸ë“œì—ì„œ 1íšŒ ì‹¤í–‰
.\scripts\worker_setup.ps1 -MasterIP "ë§ˆìŠ¤í„°_IP"
```

**ìˆ˜í–‰ ì‘ì—…:**
- PowerShell Remoting í™œì„±í™”
- ë°©í™”ë²½ ì„¤ì • (í¬íŠ¸ 29500)
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í˜¸ìŠ¤íŠ¸ ì„¤ì •
- ì „ì› ê´€ë¦¬ ìµœì í™”

#### 9.1.2 2ë‹¨ê³„: ë§ˆìŠ¤í„° ë°°í¬ (scripts/master_deploy.ps1)
```powershell
# ë§ˆìŠ¤í„° ë…¸ë“œì—ì„œ ì‹¤í–‰
.\scripts\master_deploy.ps1
```

**ìˆ˜í–‰ ì‘ì—…:**
- ì›Œì»¤ IP ìë™ íƒì§€/ì…ë ¥
- ì½”ë“œ ë° ë°ì´í„° ë³µì‚¬ (SMB/SCP)
- uv ì„¤ì¹˜ ë° ì˜ì¡´ì„± ì„¤ì¹˜
- Accelerate ì„¤ì • íŒŒì¼ ìƒì„±
- ë¶„ì‚° í›ˆë ¨ ì‹¤í–‰

### 9.2 Accelerate ì„¤ì •

#### 9.2.1 ë‹¨ì¼ GPU
```yaml
# configs/accelerate/single_gpu.yaml
compute_environment: LOCAL_MACHINE
distributed_type: 'NO'
mixed_precision: fp16
num_processes: 1
```

#### 9.2.2 ë™ì  ë©€í‹°ë…¸ë“œ ì„¤ì •
```yaml
# ëŸ°íƒ€ì„ì— ìë™ ìƒì„±
distributed_type: MULTI_GPU
num_machines: <ì›Œì»¤ìˆ˜+1>
num_processes: <ì´GPUìˆ˜>
machine_rank: <ë…¸ë“œë³„ê°œë³„ì„¤ì •>
main_process_ip: <ë§ˆìŠ¤í„°IP>
main_process_port: 29500
```

---

## 10. í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê³ ê¸‰ ê¸°ëŠ¥

### 10.1 CLIP 77í† í° ì œí•œ ë¬¸ì œ
**ë¬¸ì œ**: êµ¬ì¡°í™”ëœ ê¸´ í…ìŠ¤íŠ¸ê°€ 77í† í°ì„ ì´ˆê³¼
**í•´ê²°**: 3ê°€ì§€ ë°©ë²• ì œê³µ

### 10.2 ë°©ë²• 1: ìŠ¤ë§ˆíŠ¸ ìë¥´ê¸° (ê¸°ë³¸ê°’)
```python
# ì¤‘ìš”ë„ ê¸°ë°˜ ì •ë³´ ë³´ì¡´
priorities = [
    "Number and Type of Rooms",     # ë†’ìŒ
    "Connection Between Rooms",     # ì¤‘ê°„
    "Positional Relationship"       # ë‚®ìŒ
]
```

### 10.3 ë°©ë²• 2: Attention ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹
```python
class CLIPTextChunker:
    def chunk_text(self, text, max_length=77):
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
class CLIPTextEmbeddingAggregator:
    def __init__(self, aggregation_method="attention"):  # ê¸°ë³¸ê°’
        self.aggregation_method = aggregation_method
    
    def aggregate_embeddings(self, embeddings, chunk_lengths):
        if self.aggregation_method == "attention":
            return self._attention_aggregation(embeddings, chunk_lengths)
```

#### 10.3.1 Attention ì§‘ê³„ ì›ë¦¬
```python
def _attention_aggregation(self, embeddings, chunk_lengths):
    # 1. ê° ì²­í¬ ì„ë² ë”©ì— ëŒ€í•´ attention ì ìˆ˜ ê³„ì‚°
    attention_scores = self._compute_attention_scores(embeddings)
    
    # 2. Softmaxë¡œ ì •ê·œí™”
    attention_weights = F.softmax(attention_scores, dim=0)
    
    # 3. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì„ë² ë”© ìƒì„±
    final_embedding = torch.sum(attention_weights * embeddings, dim=0)
    return final_embedding
```

### 10.4 ë°©ë²• 3: í™•ì¥ëœ CLIP (ê°œë…ì )
```python
# src/models/extended_clip.py
# ìœ„ì¹˜ ì„ë² ë”© í™•ì¥ìœ¼ë¡œ ë” ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬
class ExtendedCLIPTextModel:
    def extend_position_embeddings(self, new_max_length):
        # ê¸°ì¡´ ìœ„ì¹˜ ì„ë² ë”© ë³´ê°„/ë³µì œ
```

### 10.5 ì„±ëŠ¥ ë¹„êµ
| ë°©ë²• | ì •ë³´ ë³´ì¡´ìœ¨ | ì²˜ë¦¬ ì‹œê°„ | í’ˆì§ˆ | ë©”ëª¨ë¦¬ |
|------|-------------|-----------|------|--------|
| ê¸°ë³¸ ìë¥´ê¸° | 30% | 1x | ë‚®ìŒ | ë‚®ìŒ |
| ìŠ¤ë§ˆíŠ¸ ìë¥´ê¸° | 80% | 1x | ì¤‘ê°„ | ë‚®ìŒ |
| í…ìŠ¤íŠ¸ ì²­í‚¹ | 100% | 5x | ë†’ìŒ | ì¤‘ê°„ |

---

## 11. ë°œìƒí•œ ë¬¸ì œë“¤ê³¼ í•´ê²°ë°©ë²•

### 11.1 ì´ˆê¸° ì„¤ì • ë¬¸ì œë“¤

#### 11.1.1 uv ì˜ì¡´ì„± ì¶©ëŒ
**ë¬¸ì œ**: `flake8`ì˜ Python ë²„ì „ ìš”êµ¬ì‚¬í•­ ì¶©ëŒ
**í•´ê²°**: `pyproject.toml`ì—ì„œ `requires-python = ">=3.9"`ë¡œ ìˆ˜ì •

#### 11.1.2 hatchling ë¹Œë“œ ì˜¤ë¥˜
**ë¬¸ì œ**: íŒ¨í‚¤ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
**í•´ê²°**: `pyproject.toml`ì— `packages = ["src"]` ì¶”ê°€

### 11.2 ëª¨ë“ˆ ì„í¬íŠ¸ ë¬¸ì œë“¤

#### 11.2.1 FloorPlanDataLoader ì„í¬íŠ¸ ì˜¤ë¥˜
**ë¬¸ì œ**: `ImportError: cannot import name 'FloorPlanDataLoader'`
**í•´ê²°**: `src/data/__init__.py`ì˜ `__all__`ì— ì¶”ê°€

#### 11.2.2 ìˆœí™˜ ì„í¬íŠ¸ ë¬¸ì œ
**ë¬¸ì œ**: `FloorPlanGenerator` ìˆœí™˜ ì„í¬íŠ¸
**í•´ê²°**: `src/inference/__init__.py`ì—ì„œ ì§ì ‘ ì„í¬íŠ¸ ì œê±°

### 11.3 LoRA ì„¤ì • ë¬¸ì œë“¤

#### 11.3.1 ì˜ëª»ëœ task_type
**ë¬¸ì œ**: `AttributeError: 'LoraModel' object has no attribute 'prepare_inputs_for_generation'`
**í•´ê²°**: `LoraConfig`ì—ì„œ `task_type="CAUSAL_LM"` ì œê±°

#### 11.3.2 íŒŒë¼ë¯¸í„° ì ‘ê·¼ ì˜¤ë¥˜
**ë¬¸ì œ**: `AttributeError: 'StableDiffusionPipeline' object has no attribute 'named_parameters'`
**í•´ê²°**: `pipeline.unet.named_parameters()` ì§ì ‘ ì ‘ê·¼

### 11.4 ë°ì´í„° ì²˜ë¦¬ ë¬¸ì œë“¤

#### 11.4.1 ë°°ì¹˜ í‚¤ ë¶ˆì¼ì¹˜
**ë¬¸ì œ**: `KeyError: 'pixel_values'`
**í•´ê²°**: ë°ì´í„°ì…‹ì—ì„œ `'image'`, `'text'` í‚¤ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •

#### 11.4.2 Windows ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜
**ë¬¸ì œ**: `RuntimeError: An attempt has been made to start a new process`
**í•´ê²°**: `num_workers: 0` ì„¤ì • (Windows ì „ìš©)

### 11.5 ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œë“¤

#### 11.5.1 U-Net ì…ë ¥ ì±„ë„ ì˜¤ë¥˜
**ë¬¸ì œ**: `expected input[4, 3, 256, 256] to have 4 channels, but got 3 channels`
**í•´ê²°**: VAE ì¸ì½”ë”©ìœ¼ë¡œ RGB â†’ ì ì¬ê³µê°„ ë³€í™˜ ì¶”ê°€

#### 11.5.2 í•™ìŠµë¥  íƒ€ì… ì˜¤ë¥˜
**ë¬¸ì œ**: `TypeError: '<=' not supported between instances of 'float' and 'str'`
**í•´ê²°**: YAMLì—ì„œ `1e-4` â†’ `0.0001` ë³€ê²½

### 11.6 Accelerate ì²´í¬í¬ì¸íŠ¸ ë¬¸ì œë“¤

#### 11.6.1 ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶ˆì¼ì¹˜
**ë¬¸ì œ**: LoRA ê°œë³„ íŒŒì¼ vs Accelerate í†µí•© ì²´í¬í¬ì¸íŠ¸
**í•´ê²°**: `AccelerateFloorPlanGenerator`ì—ì„œ `FloorPlanTrainer` í†µí•´ ë¡œë”©

#### 11.6.2 ì´ë¯¸ì§€ ë³€í™˜ íƒ€ì… ì˜¤ë¥˜
**ë¬¸ì œ**: `AttributeError: 'list' object has no attribute 'squeeze'`
**í•´ê²°**: íŒŒì´í”„ë¼ì¸ ì¶œë ¥ í˜•ì‹ ì²˜ë¦¬ ë¡œì§ ê°œì„ 

### 11.7 í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œë“¤

#### 11.7.1 numpy ë°°ì—´ ë©”ì„œë“œ ì˜¤ë¥˜
**ë¬¸ì œ**: `AttributeError: 'numpy.ndarray' object has no attribute 'dim'`
**í•´ê²°**: `image.dim()` â†’ `image.ndim` ë³€ê²½

#### 11.7.2 ì–´í…ì…˜ ì°¨ì› ë¶ˆì¼ì¹˜
**ë¬¸ì œ**: `IndexError: tuple index out of range`
**í•´ê²°**: ìŠ¤ì¹¼ë¼ attention_scores ì²˜ë¦¬ ë¡œì§ ì¶”ê°€

---

## 12. íŒŒì¼ë³„ ìƒì„¸ ì„¤ëª…

### 12.1 í•µì‹¬ ì„¤ì • íŒŒì¼

#### pyproject.toml
```toml
[project]
name = "floorplan-diffusion"
version = "0.1.0"
requires-python = ">=3.9"
dependencies = [
    "torch>=2.0.0",
    "diffusers>=0.21.0",
    "transformers>=4.30.0",
    "accelerate>=0.20.0",
    "peft>=0.4.0",
    # ... ê¸°íƒ€ ì˜ì¡´ì„±
]

[tool.hatch.build.targets.wheel]
packages = ["src"]
```

#### configs/train_config.yaml
```yaml
training:
  model_name: "runwayml/stable-diffusion-v1-5"
  lora_rank: 16
  lora_alpha: 32
  learning_rate: 1e-4
  batch_size: 2
  num_epochs: 100
  mixed_precision: "fp16"

data:
  train_text_dir: "./data/train/input_text"
  train_image_dir: "./data/train/label_floorplan"
  num_workers: 0  # Windows í˜¸í™˜ì„±
```

### 12.2 í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ

#### src/data/dataset.py
```python
class FloorPlanDataset(Dataset):
    def __init__(self, text_dir, image_dir, tokenizer, transform=None):
        self.text_files = sorted(glob.glob(os.path.join(text_dir, "*.txt")))
        self.image_files = sorted(glob.glob(os.path.join(image_dir, "*.png")))
        self.tokenizer = tokenizer
        
    def __getitem__(self, idx):
        # í…ìŠ¤íŠ¸ ë¡œë”© ë° í† í°í™”
        with open(self.text_files[idx], 'r', encoding='utf-8') as f:
            text = f.read().strip()
        
        # ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
        image = Image.open(self.image_files[idx]).convert('RGB')
        
        return {
            'text': text,
            'image': self.transform(image) if self.transform else image
        }
```

#### src/data/text_chunking.py
```python
class CLIPTextEmbeddingAggregator:
    def __init__(self, aggregation_method="attention"):  # ê¸°ë³¸ê°’: attention
        self.aggregation_method = aggregation_method
    
    def _attention_aggregation(self, embeddings, chunk_lengths):
        # Query: ì „ì²´ ì„ë² ë”©ì˜ í‰ê· 
        query = torch.mean(embeddings, dim=0, keepdim=True)
        
        # Key = Value: ê° ì²­í¬ ì„ë² ë”©
        keys = values = embeddings
        
        # Attention ì ìˆ˜ ê³„ì‚°
        attention_scores = torch.matmul(query, keys.transpose(0, 1))
        attention_weights = F.softmax(attention_scores, dim=1)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì„ë² ë”© ìƒì„±
        final_embedding = torch.matmul(attention_weights, values)
        return final_embedding.squeeze(0)
```

#### src/models/lora_wrapper.py
```python
class LoRAWrapper:
    def __init__(self, pipeline, lora_rank=16, lora_alpha=32):
        # U-Net LoRA ì„¤ì •
        unet_lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=lora_alpha,
            target_modules=["to_q", "to_k", "to_v", "to_out.0"],
            lora_dropout=0.1,
        )
        self.pipeline.unet = get_peft_model(self.pipeline.unet, unet_lora_config)
        
        # Text Encoder LoRA ì„¤ì •
        text_encoder_lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=lora_alpha,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=0.1,
        )
        self.pipeline.text_encoder = get_peft_model(self.pipeline.text_encoder, text_encoder_lora_config)
```

#### src/training/trainer.py
```python
class FloorPlanTrainer:
    def __init__(self, config):
        self._setup_accelerator()
        self.model = FloorPlanDiffusionModel(config)
        self._prepare_accelerate()
    
    def _setup_accelerator(self):
        self.accelerator = Accelerator(
            mixed_precision=self.config.get('mixed_precision', 'fp16'),
            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 1),
            project_config=ProjectConfiguration(project_dir="./checkpoints"),
        )
    
    def _train_step_accelerate(self, batch):
        # VAE ì¸ì½”ë”©: RGB â†’ ì ì¬ê³µê°„
        with torch.no_grad():
            latents = self.model.pipeline.vae.encode(batch['image']).latent_dist.sample()
            latents = latents * 0.18215
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device)
        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_inputs = self.tokenizer(batch['text'], max_length=77, truncation=True, return_tensors="pt")
        encoder_hidden_states = self.model.pipeline.text_encoder(text_inputs.input_ids.to(latents.device))[0]
        
        # U-Net ì˜ˆì¸¡
        noise_pred = self.model.pipeline.unet(noisy_latents, timesteps, encoder_hidden_states).sample
        
        # ì†ì‹¤ ê³„ì‚°
        loss = F.mse_loss(noise_pred, noise)
        return loss
```

#### src/inference/generator.py
```python
class AccelerateFloorPlanGenerator:
    def __init__(self, checkpoint_path, config_path):
        self.config = self._load_config(config_path)
        self.trainer = FloorPlanTrainer(self.config)
        self.trainer.load_checkpoint(checkpoint_path)
        
    def generate_single(self, text, num_inference_steps=50, guidance_scale=7.5, seed=None):
        if seed is not None:
            torch.manual_seed(seed)
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        result = self.trainer.model.pipeline(
            text,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            return_dict=True
        )
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë³€í™˜
        if isinstance(result, dict) and 'images' in result:
            image = result['images'][0]
        else:
            image = result[0] if isinstance(result, list) else result
        
        # PIL.Image â†’ numpy ë³€í™˜
        if hasattr(image, 'convert'):
            image = np.array(image.convert('RGB'))
        
        return image
```

### 12.3 ë¶„ì‚°í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

#### scripts/worker_setup.ps1
```powershell
param(
    [Parameter(Mandatory=$true)]
    [string]$MasterIP
)

# PowerShell Remoting í™œì„±í™”
Enable-PSRemoting -Force -SkipNetworkProfileCheck

# ë°©í™”ë²½ ê·œì¹™ ì¶”ê°€
New-NetFirewallRule -DisplayName "PyTorch Distributed" -Direction Inbound -Protocol TCP -LocalPort 29500 -Action Allow

# ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í˜¸ìŠ¤íŠ¸ ì„¤ì •
Set-Item WSMan:\localhost\Client\TrustedHosts -Value $MasterIP -Force

# ì „ì› ê´€ë¦¬ ìµœì í™”
powercfg /setactive 8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c  # ê³ ì„±ëŠ¥

Write-Host "ì›Œì»¤ ë…¸ë“œ ì„¤ì • ì™„ë£Œ: $env:COMPUTERNAME"
```

#### scripts/master_deploy.ps1
```powershell
# ì›Œì»¤ IP ìˆ˜ì§‘
$WorkerIPs = @()
if ($WorkerIPs.Count -eq 0) {
    # ëŒ€í™”í˜• ì…ë ¥
    do {
        $ip = Read-Host "ì›Œì»¤ ë…¸ë“œ IP ì…ë ¥ (ì™„ë£Œì‹œ Enter)"
        if ($ip) { $WorkerIPs += $ip }
    } while ($ip)
}

# ê° ì›Œì»¤ì— ë°°í¬
foreach ($workerIP in $WorkerIPs) {
    # ì½”ë“œ ë³µì‚¬
    Copy-Item -Path ".\*" -Destination "\\$workerIP\C$\distributed_training\" -Recurse -Force
    
    # ì›ê²© ì‹¤í–‰
    Invoke-Command -ComputerName $workerIP -ScriptBlock {
        cd C:\distributed_training
        
        # uv ì„¤ì¹˜
        if (!(Get-Command uv -ErrorAction SilentlyContinue)) {
            pip install uv
        }
        
        # ì˜ì¡´ì„± ì„¤ì¹˜
        uv sync
    }
}

# Accelerate ì„¤ì • ìƒì„± ë° ë¶„ì‚° í›ˆë ¨ ì‹œì‘
$masterIP = (Get-NetIPAddress -AddressFamily IPv4 | Where-Object {$_.IPAddress -like "192.168.*"}).IPAddress
$numMachines = $WorkerIPs.Count + 1
$numProcesses = $numMachines * 2  # ë…¸ë“œë‹¹ 2 GPU ê°€ì •

# ë§ˆìŠ¤í„° ë…¸ë“œ í›ˆë ¨ ì‹œì‘
accelerate launch --config_file "configs/accelerate/master_config.yaml" scripts/train.py
```

### 12.4 í…ŒìŠ¤íŠ¸ ë° ìœ í‹¸ë¦¬í‹°

#### scripts/test_token_limits.py
```python
def main():
    # í…ìŠ¤íŠ¸ ì²­í‚¹ í…ŒìŠ¤íŠ¸
    chunker = CLIPTextChunker()
    aggregator = CLIPTextEmbeddingAggregator()  # ê¸°ë³¸ê°’: attention
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    methods = ["truncate", "chunk", "smart_truncate"]
    for method in methods:
        start_time = time.time()
        result = process_text(sample_text, method)
        elapsed = time.time() - start_time
        print(f"{method}: {elapsed:.3f}ì´ˆ")
```

---

## 13. ì„¤ì • ë° ì‹¤í–‰ ë°©ë²•

### 13.1 ì´ˆê¸° ì„¤ì •
```bash
# 1. í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository-url>
cd text-to-image-generation

# 2. uv ì„¤ì¹˜ ë° ì˜ì¡´ì„± ì„¤ì¹˜
pip install uv
uv sync

# 3. ê°€ìƒí™˜ê²½ í™œì„±í™” (Windows)
.\.venv\Scripts\activate
```

### 13.2 ë‹¨ì¼ GPU í›ˆë ¨
```bash
python scripts/train.py --config configs/train_config.yaml
```

### 13.3 ë©€í‹°ë…¸ë“œ ë¶„ì‚°í›ˆë ¨ (Windows)
```powershell
# 1ë‹¨ê³„: ê° ì›Œì»¤ì—ì„œ 1íšŒ ì‹¤í–‰
.\scripts\worker_setup.ps1 -MasterIP "ë§ˆìŠ¤í„°_IP"

# 2ë‹¨ê³„: ë§ˆìŠ¤í„°ì—ì„œ ì‹¤í–‰
.\scripts\master_deploy.ps1
```

### 13.4 í‰ë©´ë„ ìƒì„±
```bash
# ë‹¨ì¼ ìƒì„±
python scripts/generate.py --checkpoint checkpoints/checkpoints --text "SJH-Style FloorPlan Generation [Number and Type of Rooms] The floorplan have 1 living room, 1 kitchen"

# ëŒ€í™”í˜• ìƒì„±
python scripts/generate.py --checkpoint checkpoints/checkpoints --interactive
```

### 13.5 í…ìŠ¤íŠ¸ ì²­í‚¹ í…ŒìŠ¤íŠ¸
```bash
python scripts/test_token_limits.py
```

---

## 14. ê°œë°œ íˆìŠ¤í† ë¦¬

### 14.1 Phase 1: ê¸°ë³¸ êµ¬í˜„ (ì´ˆê¸°)
- Stable Diffusion + LoRA ê¸°ë³¸ íŒŒì¸íŠœë‹ êµ¬í˜„
- ë°ì´í„°ì…‹ ë° ê¸°ë³¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ì´ˆê¸° ë°ì´í„° ì¦ê°• ì „ëµ ê°œë°œ

### 14.2 Phase 2: Accelerate ì „í™˜
- `torch.distributed` â†’ Hugging Face Accelerate ë§ˆì´ê·¸ë ˆì´ì…˜
- í†µí•© í›ˆë ¨ê¸° (`FloorPlanTrainer`) ê°œë°œ
- ë¶„ì‚°í•™ìŠµ ì§€ì› êµ¬ì¶•

### 14.3 Phase 3: ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•
- `AccelerateFloorPlanGenerator` ê°œë°œ
- Accelerate ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„± êµ¬í˜„
- í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### 14.4 Phase 4: í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê³ ë„í™”
- CLIP 77í† í° ì œí•œ ë¬¸ì œ í•´ê²°
- Attention ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹ êµ¬í˜„
- 3ê°€ì§€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë°©ë²• ì œê³µ

### 14.5 Phase 5: ë¶„ì‚°í•™ìŠµ ìë™í™”
- Windows í™˜ê²½ ìµœì í™”
- 2ë‹¨ê³„ ë¶„ì‚°í•™ìŠµ ë°©ì‹ ê°œë°œ
- PowerShell ê¸°ë°˜ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

### 14.6 Phase 6: í”„ë¡œì íŠ¸ ì •ë¦¬
- ë¯¸ì‚¬ìš© íŒŒì¼ ì •ë¦¬
- ë¬¸ì„œ ì²´ê³„ ì •ë¹„
- ì¢…í•© ê°€ì´ë“œ ì‘ì„±

---

## ğŸ¯ í•µì‹¬ í˜ì‹ ì‚¬í•­ ìš”ì•½

1. **Attention ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹**: CLIP 77í† í° ì œí•œì„ ìš°ì•„í•˜ê²Œ í•´ê²°
2. **2ë‹¨ê³„ ë¶„ì‚°í•™ìŠµ**: Windows í™˜ê²½ì— ìµœì í™”ëœ ìë™í™” ì‹œìŠ¤í…œ
3. **í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ**: Accelerate ê¸°ë°˜ ë‹¨ì¼ ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë“  í™˜ê²½ ì§€ì›
4. **ì§€ëŠ¥ì  ë°ì´í„° ì¦ê°•**: ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ì„ ë°˜ì˜í•œ ë¶€ë¶„ ì¡°ê±´ ì²˜ë¦¬

---

**ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ë©°, ìƒˆë¡œìš´ ì„¸ì…˜ì—ì„œ ì¦‰ì‹œ ê°œë°œì„ ì´ì–´ê°ˆ ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.** ğŸš€
